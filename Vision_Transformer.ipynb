{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLfm_EITwC4k"
      },
      "source": [
        "# **Vision Transformer (ViT)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MHDeH8Ydx7kq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import timeit\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zibb1jZiyPGM"
      },
      "outputs": [],
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "  def __init__(self,\n",
        "               in_channels=3,\n",
        "               patch_size=4,\n",
        "               embedding_dim=48):\n",
        "    super().__init__()\n",
        "\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "    self.patcher = nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x_patched = self.patcher(x)\n",
        "    x_flattened = self.flatten(x_patched)\n",
        "\n",
        "    print(\"Flattened vector: \", x_flattened.shape)\n",
        "\n",
        "    return x_flattened.permute(0, 2, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jt1dYkUaqMk5",
        "outputId": "a11fab64-ba2a-4c33-d447-6f1ab08a0252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random image shape: torch.Size([64, 3, 32, 32])\n",
            "Flattened vector:  torch.Size([64, 48, 64])\n",
            "Patch Embeddings output: torch.Size([64, 64, 48])\n"
          ]
        }
      ],
      "source": [
        "#testing the patcher\n",
        "random_example = torch.randn(64,3,32,32)\n",
        "print(\"Random image shape:\", random_example.shape)\n",
        "\n",
        "patcher = PatchEmbeddings()\n",
        "patcher_output = patcher(random_example)\n",
        "print(\"Patch Embeddings output:\", patcher_output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lFo2ceFCrO4U"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               img_size=32,\n",
        "               patch_size=4,\n",
        "               embedding_dim=48,\n",
        "               in_channels=3,\n",
        "               dropout=0.1,\n",
        "               num_att_heads=8,\n",
        "               mlp_size = 3072,\n",
        "               num_transformer_layer=12,\n",
        "               num_classes = 10):\n",
        "    super().__init__()\n",
        "\n",
        "    assert img_size % patch_size == 0, f\"Image size must be divisble by patch size!!! Image shape is: {img_size}, while patch size is: {patch_size}\"\n",
        "\n",
        "    self.patch_embeddings = PatchEmbeddings(in_channels=in_channels,\n",
        "                                            patch_size=patch_size,\n",
        "                                            embedding_dim=embedding_dim)\n",
        "\n",
        "    self.class_token = nn.Parameter(torch.randn(size=(1, 1, embedding_dim)), requires_grad=True)\n",
        "\n",
        "    num_patches = (img_size // patch_size) * (img_size // patch_size) # determine the number of patches\n",
        "\n",
        "    self.positional_embedding = nn.Parameter(torch.rand(size=(1, num_patches + 1, embedding_dim)),requires_grad=True)\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    # single transformer encoder layer\n",
        "    self.transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
        "                                                                nhead=num_att_heads,\n",
        "                                                                dim_feedforward=mlp_size,\n",
        "                                                                dropout=dropout,\n",
        "                                                                activation=\"gelu\",\n",
        "                                                                batch_first=True,\n",
        "                                                                norm_first=True)\n",
        "\n",
        "    # stack of N transformer encoder layers\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer=self.transformer_encoder_layer, num_layers=num_transformer_layer)\n",
        "\n",
        "    # MLP head\n",
        "    self.mlp_head = nn.Sequential(\n",
        "        nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "        nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    patch_embeddings = self.patch_embeddings(x)\n",
        "\n",
        "    batch_size = x.shape[0]\n",
        "    class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "\n",
        "    x = torch.cat((class_token, patch_embeddings), dim=1)\n",
        "\n",
        "    x = self.positional_embedding + x\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.transformer_encoder(x)\n",
        "\n",
        "    x = self.mlp_head(x[:, 0]) # passing just the class token through the MLP head to get final classification\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2kX2vMMVHt9I"
      },
      "outputs": [],
      "source": [
        "cifar10_labels = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbS0-JW0yQdD",
        "outputId": "bdcc6ffd-4496-409c-ca13-bfb19d5306c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random image shape: torch.Size([1, 3, 32, 32])\n",
            "Flattened vector:  torch.Size([1, 48, 64])\n",
            "The output of Vision Transformer: tensor([[-0.6941,  0.3187,  0.2410, -0.5704,  0.4219, -0.1650,  0.0610, -1.1321,\n",
            "         -0.2041, -1.1518]], grad_fn=<AddmmBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "img_example = torch.randn(1, 3, 32, 32)\n",
        "print(\"Random image shape:\", img_example.shape)\n",
        "\n",
        "# Create ViT\n",
        "vision_transformer = VisionTransformer(num_classes=len(cifar10_labels))\n",
        "print(\"The output of Vision Transformer:\", vision_transformer(img_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tit9cRk29F3m",
        "outputId": "eadd6008-7a32-479e-a630-9ec89e583773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_nQ4Bhi8FPW",
        "outputId": "82040cd9-1dbf-479d-b01d-ec22a9367afb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flattened vector:  torch.Size([1, 48, 64])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "VisionTransformer                             [1, 10]                   310,800\n",
              "├─PatchEmbeddings: 1-1                        [1, 64, 48]               --\n",
              "│    └─Conv2d: 2-1                            [1, 48, 8, 8]             2,352\n",
              "│    └─Flatten: 2-2                           [1, 48, 64]               --\n",
              "├─Dropout: 1-2                                [1, 65, 48]               --\n",
              "├─TransformerEncoder: 1-3                     [1, 65, 48]               --\n",
              "│    └─ModuleList: 2-3                        --                        --\n",
              "│    │    └─TransformerEncoderLayer: 3-1      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-2      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-3      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-4      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-5      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-6      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-7      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-8      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-9      [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-10     [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-11     [1, 65, 48]               307,632\n",
              "│    │    └─TransformerEncoderLayer: 3-12     [1, 65, 48]               307,632\n",
              "├─Sequential: 1-4                             [1, 10]                   --\n",
              "│    └─LayerNorm: 2-4                         [1, 48]                   96\n",
              "│    └─Linear: 2-5                            [1, 10]                   490\n",
              "===============================================================================================\n",
              "Total params: 4,005,322\n",
              "Trainable params: 4,005,322\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 3.73\n",
              "===============================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 20.09\n",
              "Params size (MB): 14.33\n",
              "Estimated Total Size (MB): 34.43\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(vision_transformer, input_size=img_example.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bizupaxu167m",
        "outputId": "73d58a93-7f74-4b5b-8a86-352d0eaa5c27"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wll4iuj1cenU"
      },
      "source": [
        "# **Pretained ViT-base-16**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ1FI-uyffYE",
        "outputId": "d451be82-48d2-4b3b-c454-7570f651a7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:05<00:00, 68.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "===============================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape              Param #\n",
              "===============================================================================================\n",
              "VisionTransformer                             [1, 10]                   768\n",
              "├─Conv2d: 1-1                                 [1, 768, 14, 14]          (590,592)\n",
              "├─Encoder: 1-2                                [1, 197, 768]             151,296\n",
              "│    └─Dropout: 2-1                           [1, 197, 768]             --\n",
              "│    └─Sequential: 2-2                        [1, 197, 768]             --\n",
              "│    │    └─EncoderBlock: 3-1                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-2                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-3                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-4                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-5                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-6                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-7                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-8                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-9                 [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-10                [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-11                [1, 197, 768]             (7,087,872)\n",
              "│    │    └─EncoderBlock: 3-12                [1, 197, 768]             (7,087,872)\n",
              "│    └─LayerNorm: 2-3                         [1, 197, 768]             (1,536)\n",
              "├─Sequential: 1-3                             [1, 10]                   --\n",
              "│    └─LayerNorm: 2-4                         [1, 768]                  1,536\n",
              "│    └─Linear: 2-5                            [1, 10]                   7,690\n",
              "===============================================================================================\n",
              "Total params: 85,807,882\n",
              "Trainable params: 9,226\n",
              "Non-trainable params: 85,798,656\n",
              "Total mult-adds (M): 172.47\n",
              "===============================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 104.09\n",
              "Params size (MB): 229.23\n",
              "Estimated Total Size (MB): 333.92\n",
              "==============================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Feature Extraction of ViT-base-16, I am freezing all inner layer while I change only the last layer\n",
        "\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights)\n",
        "\n",
        "\n",
        "# Freeze all layers in pretrained ViT model\n",
        "for param in pretrained_vit.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "\n",
        "# Update just the last layer of pretrained ViT, making it suitable for CIFAR-10 classification\n",
        "embedding_dim = 768\n",
        "pretrained_vit.heads = nn.Sequential(\n",
        "    nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "    nn.Linear(in_features=embedding_dim,\n",
        "             out_features=len(cifar10_labels))\n",
        ")\n",
        "\n",
        "pretrained_vit.to(device)\n",
        "\n",
        "summary(model=pretrained_vit,\n",
        "        input_size=(1, 3, 224, 224)) # ViT-base-16 accepts 224x224 size images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUKqsFWt6yov"
      },
      "source": [
        "# **Processing CIFAR-10 data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz3XqsnwKbnZ",
        "outputId": "0407a164-e39e-4764-eaa9-837a4acc9bfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:18<00:00, 9.05MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Training dataset size is: 27000\n",
            "Validation dataset size is: 3000\n"
          ]
        }
      ],
      "source": [
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "\n",
        "full_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=pretrained_vit_transforms)\n",
        "\n",
        "fraction = 0.6  # fraction of the dataset to use for training and validation\n",
        "num_train_samples = int(len(full_train_dataset) * fraction)\n",
        "random_seed = 42  # for reproducibility\n",
        "\n",
        "torch.manual_seed(random_seed)\n",
        "random_indices = torch.randperm(len(full_train_dataset))[:num_train_samples]\n",
        "\n",
        "train_subset = Subset(full_train_dataset, random_indices)\n",
        "\n",
        "# spliting the subset into training (90%) and validation (10%) sets\n",
        "train_indices, val_indices = train_test_split(range(len(train_subset)), test_size=0.1, random_state=random_seed, shuffle=True)\n",
        "\n",
        "train_dataset = Subset(train_subset, train_indices)\n",
        "val_dataset = Subset(train_subset, val_indices)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "print(f\"Training dataset size is: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size is: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP2fxOfYZHw1"
      },
      "source": [
        "# **Feature Extraction of Pre-Trained ViT-base-16(i.e., training just the head(last layer) for CIFAR-10 classification)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW3gIHYigKdN",
        "outputId": "d01c9687-b7c7-41b6-9bd3-da6992297001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:10<00:00,  1.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Train Loss: 0.3168, Train Accuracy: 0.9060\n",
            "Epoch 1/15, Val Loss: 0.1776, Val Accuracy: 0.9420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:14<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15, Train Loss: 0.1635, Train Accuracy: 0.9464\n",
            "Epoch 2/15, Val Loss: 0.1586, Val Accuracy: 0.9443\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:14<00:00,  1.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15, Train Loss: 0.1389, Train Accuracy: 0.9546\n",
            "Epoch 3/15, Val Loss: 0.1461, Val Accuracy: 0.9487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:15<00:00,  1.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15, Train Loss: 0.1236, Train Accuracy: 0.9609\n",
            "Epoch 4/15, Val Loss: 0.1445, Val Accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:14<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15, Train Loss: 0.1133, Train Accuracy: 0.9633\n",
            "Epoch 5/15, Val Loss: 0.1433, Val Accuracy: 0.9487\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:14<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15, Train Loss: 0.1042, Train Accuracy: 0.9661\n",
            "Epoch 6/15, Val Loss: 0.1470, Val Accuracy: 0.9490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:13<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/15, Train Loss: 0.0975, Train Accuracy: 0.9684\n",
            "Epoch 7/15, Val Loss: 0.1430, Val Accuracy: 0.9503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:16<00:00,  1.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15, Train Loss: 0.0923, Train Accuracy: 0.9710\n",
            "Epoch 8/15, Val Loss: 0.1444, Val Accuracy: 0.9500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:15<00:00,  1.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/15, Train Loss: 0.0868, Train Accuracy: 0.9720\n",
            "Epoch 9/15, Val Loss: 0.1501, Val Accuracy: 0.9493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:13<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15, Train Loss: 0.0824, Train Accuracy: 0.9733\n",
            "Epoch 10/15, Val Loss: 0.1464, Val Accuracy: 0.9520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:13<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15, Train Loss: 0.0781, Train Accuracy: 0.9749\n",
            "Epoch 11/15, Val Loss: 0.1490, Val Accuracy: 0.9507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:13<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15, Train Loss: 0.0750, Train Accuracy: 0.9767\n",
            "Epoch 12/15, Val Loss: 0.1511, Val Accuracy: 0.9473\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:12<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/15, Train Loss: 0.0711, Train Accuracy: 0.9781\n",
            "Epoch 13/15, Val Loss: 0.1542, Val Accuracy: 0.9497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:13<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/15, Train Loss: 0.0693, Train Accuracy: 0.9781\n",
            "Epoch 14/15, Val Loss: 0.1583, Val Accuracy: 0.9470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT training for CIFAR-10 classification: 100%|██████████| 211/211 [06:12<00:00,  1.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/15, Train Loss: 0.0647, Train Accuracy: 0.9803\n",
            "Epoch 15/15, Val Loss: 0.1575, Val Accuracy: 0.9497\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n",
        "                             lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    pretrained_vit.train()\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"ViT training for CIFAR-10 classification\"):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = pretrained_vit(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        total_train += labels.size(0)\n",
        "\n",
        "    train_accuracy = train_correct / total_train\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
        "\n",
        "    pretrained_vit.eval()\n",
        "    val_loss = 0\n",
        "    val_correct = 0\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images, labels = batch\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = pretrained_vit(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            total_val += labels.size(0)\n",
        "\n",
        "    val_accuracy = val_correct / total_val\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBEDovlSdoOf"
      },
      "source": [
        "# **Testing new ViT on CIFAR-10 data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubzd64iG1LI9",
        "outputId": "770597f9-d9ac-4af6-c1de-a79c74c3c784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Test dataset size is: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing new ViT: 100%|██████████| 79/79 [01:09<00:00,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1240, Test Accuracy: 0.9643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=pretrained_vit_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "print(f\"Test dataset size is: {len(test_dataset)}\")\n",
        "\n",
        "pretrained_vit.eval()\n",
        "\n",
        "test_loss = 0\n",
        "test_correct = 0\n",
        "total_test = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Testing new ViT\"):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = pretrained_vit(images)\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs, 1)\n",
        "\n",
        "        test_correct += (preds == labels).sum().item()\n",
        "        total_test += labels.size(0)\n",
        "\n",
        "test_accuracy = test_correct / total_test\n",
        "print(f\"Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {test_accuracy:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}